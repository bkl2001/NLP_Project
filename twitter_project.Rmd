---
title: "Twitter Project - Uber vs. Lyft"
author: "Bayleigh Logan"
output: 
  html_document:
    theme: yeti
    highlight: pygments
    toc: true
    number_sections: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
---

# Getting Twitter API and Authorization
```{r, eval=FALSE}

#Obtaining the consumer_key, consume_secret, access_token, and access_secret from Twitter API
access_token <- "YOUR_ACCESS_TOKEN"
access_token_secret <- "YOUR_ACCESS_TOKEN_SECRET"
consumer_key <- "YOUR_CUSTOMER_KEY"
consumer_secret <- "YOUR_CUSTOMER_SECRET_KEY"

#Setting up Twitter authorization
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_token_secret)


```

```{r setup, include=FALSE, eval=FALSE}

#Installing Twitter Package
#install.packages("twitteR")

#Setting Working Directory
setwd("C:\\Users\\baylo\\Twitter Project")
getwd()


```


# Collecting Twitter Data
```{r, eval=FALSE}

#Getting Uber and Lyft related Tweets
tw1 = searchTwitter('"Uber"', n = 50000, lang = "en", since = '2019-02-02', until = '2019-02-07')
tw2 = searchTwitter('"Lyft"', n = 50000, lang="en",  since = '2019-02-02', until = '2019-02-07')

#Stripping retweets from data
no_retweets = strip_retweets(tw1, strip_manual = TRUE)
no_retweets1 = strip_retweets(tw2, strip_manual = TRUE)

#Converting tweet data to dataframe
uber = twListToDF(no_retweets)
lyft = twListToDF(no_retweets1)

#Fixing encoding of twitter text
uber$text <- iconv(uber$text, from = "UTF-8", to = "ASCII", sub="")
lyft$text <- iconv(lyft$text, from = "UTF-8", to = "ASCII", sub="")

#Saving to a csv file
write.csv(uber, file = "Uber.csv")
write.csv(lyft, file = "Lyft.csv")


```

# Cleaning the Twitter Data
```{r, eval=FALSE}

#Flagging the instances where both uber and lyft are found in the tweets
lyft$flag <- ifelse(grepl("uber|Uber", lyft$text) & grepl("lyft|Lyft", lyft$text),1,0)
uber$flag <- ifelse(grepl("uber|Uber", uber$text) & grepl("lyft|Lyft", uber$text),1,0)

#Subsetting them out of the original data
uber_lyft <- subset(uber, flag == 1)
uber_lyft1 <- subset(lyft, flag == 1)

#Combing the 2 subsets into 1 dataset
uber_and_lyft = rbind(uber_lyft, uber_lyft1)

#Saving the combination tweets to csv
write.csv(uber_and_lyft, file = "Uber_Lyft_cleaned.csv")

#Deleting them out of the original datasets for more accurate analysis
uber_final <- uber[!(uber$flag==1),]
lyft_final <- lyft[!(lyft$flag==1),]

#Saving cleaned data to a csv file
write.csv(uber_final, file = "Uber_cleaned.csv")
write.csv(lyft_final, file = "Lyft_cleaned.csv")

```

# Starting the Text Analysis
```{r, eval=FALSE, message=FALSE, warning=FALSE}

#Importing Libraries for Text Analysis
library(qdap)
library(readr)
library(tm)
library(wordcloud)
library(RWeka)
library(readtext)
library(stopwords)
library(wordcloud2)
library(dplyr)
library(ggplot2)
library(plotly)
library(twitteR)
library(text2vec)
library(tidytext)
library(radarchart)

#Importing cleaned tweet data
uber <- read.csv("Uber_cleaned.csv", row.names = 1)
lyft <- read.csv("Lyft_cleaned.csv", row.names = 1)

```

### Creating Word Count and Corpus

```{r, eval=FALSE}

#Word Count for Uber and Lyft
uber_count <- freq_terms(uber$text, top = 20)
lyft_count <- freq_terms(lyft$text, top = 20)

#Plotting the top 20 word counts
plot(uber_count)
plot(lyft_count)

#Making a Corpus of a Vector Source 
uber_corpus <- VCorpus(VectorSource(uber$text))
lyft_corpus <- VCorpus(VectorSource(lyft$text))


#Printing the 12th Tweets to explore the content
content(uber_corpus[[12]])
content(lyft_corpus[[12]])


```


# Cleaning Courpus - Pre Processing
```{r, eval=FALSE}

#tm_map() function is used to apply various preprocessing functions on the corpus
#if the preprocessing function is not from the "tm" library, it has to be wrapped around a content_transformer() function

#Creating functions to use in cleaning procress
removeMentions <- function(x) gsub("@\\S+", "", x)
removeLinks <- function(x) gsub("http.*", "", x)


clean_corpus <- function(corpus){
  cleaned_corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(removeLinks))
  cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(removeMentions))
  cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(tolower))
  cleaned_corpus <- tm_map(cleaned_corpus, removePunctuation)
  cleaned_corpus <- tm_map(cleaned_corpus, removeNumbers)
  cleaned_corpus <- tm_map(cleaned_corpus, removeWords, stopwords("english"))
  custom_stop_words <- c("rt", "uber", "lyft", "Uber", "Lyft", "ubers", "lyfts", "Ubers", "Lyfts", "miguelt","ltlt","zoot","gtgt","u","dont","can")
  cleaned_corpus <- tm_map(cleaned_corpus, removeWords, custom_stop_words)
  cleaned_corpus <- tm_map(cleaned_corpus, stripWhitespace)
  return(cleaned_corpus)
}


#Applying the clean corpus to original corpus 
cleaned_uber_corpus <- clean_corpus(uber_corpus)
cleaned_lyft_corpus <- clean_corpus(lyft_corpus)

#Printing out 12th Tweet again to see the cleaned version
print(cleaned_uber_corpus[[12]][1])
print(cleaned_lyft_corpus[[12]][1])


```

# TDM
```{r, eval=FALSE}

#Converting Cleaned Corpus to TDM
TDM_uber <- TermDocumentMatrix(cleaned_uber_corpus)
TDM_uber_m <- as.matrix(TDM_uber)

TDM_lyft <- TermDocumentMatrix(cleaned_lyft_corpus)
TDM_lyft_m <- as.matrix(TDM_lyft)

#Term Frequency
term_frequency_uber <- rowSums(TDM_uber_m)
term_frequency_lyft <- rowSums(TDM_lyft_m)

#Sorting Term Frequency in Descending Order
term_frequency_uber <- sort(term_frequency_uber,dec=TRUE)
term_frequency_lyft <- sort(term_frequency_lyft,dec=TRUE)



```

# DTM using Text2Vec
```{r, eval=FALSE}

#Creating functions to use in cleaning procress
removeMentions <- function(x) gsub("@\\S+", "", x)
removeLinks <- function(x) gsub("http.*", "", x)

#Define preprocessing function and tokenization function
prep_fun  <- function(x){
  x %>%
    replace_abbreviation %>%
    removeLinks %>%
    removeMentions %>%
    tolower %>%
    removePunctuation %>%
    removeNumbers %>%
    stripWhitespace %>%
    removeWords(c("rt", "uber", "lyft", "Uber", "Lyft", "ubers", "lyfts", "Ubers", "Lyfts", "miguelt","ltlt","zoot","gtgt","u","dont","can")) %>%
    word_tokenizer
}

#Creating tokens  
uber_tokens <- prep_fun(uber$text)
lyft_tokens <- prep_fun(lyft$text)

#Getting stopwords list
stop_words <- stopwords("english")

uber_it <- itoken(uber_tokens)
lyft_it <- itoken(lyft_tokens)

#Getting word frequencies and number of documents they appear it
uber_vocab <- create_vocabulary(uber_it,  stopwords = stop_words)
lyft_vocab <- create_vocabulary(lyft_it,  stopwords = stop_words)

#Vectorization
uber_vectorizer <- vocab_vectorizer(uber_vocab)
lyft_vectorizer <- vocab_vectorizer(lyft_vocab)

#Creating the DTM
uber_dtm <- create_dtm(uber_it , uber_vectorizer, type = "dgTMatrix")
lyft_dtm <- create_dtm(lyft_it , lyft_vectorizer, type = "dgTMatrix")

```

# TF-IDF (Term Frequency-Inverse Document Frequency)
```{r, eval=FALSE}

#Creating TFIDF 
model_tfidf = TfIdf$new()
dtm_tfidf_uber = model_tfidf$fit_transform(uber_dtm)
dtm_tfidf_lyft = model_tfidf$fit_transform(lyft_dtm)

#Transforming to a matrix
dtm_uber_m = as.matrix(dtm_tfidf_uber)
dtm_lyft_m = as.matrix(dtm_tfidf_lyft)

#Sorting the term frequencies
sort1 = sort(colSums(dtm_uber_m),decreasing=T)
sort2 = sort(colSums(dtm_lyft_m),decreasing=T)

#Creating a dataframe of terms and frequency count
tfidf_freq_uber = data.frame(words=names(sort1),freq=sort1)
tfidf_freq_lyft = data.frame(words=names(sort2),freq=sort2)


```


# Creating Bigrams and Trigrams

### Bigrams:
```{r, eval=FALSE}

bi_uber_it <- itoken(uber_tokens)
bi_lyft_it <- itoken(lyft_tokens)

bi_uber_vocab <- create_vocabulary(bi_uber_it,  stopwords = stop_words, ngram = c(ngram_min=2, ngram_max=2), sep_ngram = " ")
bi_lyft_vocab <- create_vocabulary(bi_lyft_it,  stopwords = stop_words, ngram = c(ngram_min=2, ngram_max=2), sep_ngram = " ")

bi_uber_vectorizer <- vocab_vectorizer(bi_uber_vocab)
bi_lyft_vectorizer <- vocab_vectorizer(bi_lyft_vocab)

bi_uber_dtm <- create_dtm(bi_uber_it , bi_uber_vectorizer, type = "dgTMatrix")
bi_lyft_dtm <- create_dtm(bi_lyft_it , bi_lyft_vectorizer, type = "dgTMatrix")


```


### Trigrams:
```{r, eval=FALSE}

tri_uber_it <- itoken(uber_tokens)
tri_lyft_it <- itoken(lyft_tokens)

tri_uber_vocab <- create_vocabulary(tri_uber_it,  stopwords = stop_words, ngram = c(ngram_min=3, ngram_max=3), sep_ngram = " ")
tri_lyft_vocab <- create_vocabulary(tri_lyft_it,  stopwords = stop_words, ngram = c(ngram_min=3, ngram_max=3), sep_ngram = " ")

tri_uber_vectorizer <- vocab_vectorizer(tri_uber_vocab)
tri_lyft_vectorizer <- vocab_vectorizer(tri_lyft_vocab)

tri_uber_dtm <- create_dtm(tri_uber_it , tri_uber_vectorizer, type = "dgTMatrix")
tri_lyft_dtm <- create_dtm(tri_lyft_it , tri_lyft_vectorizer, type = "dgTMatrix")

```


# Word Clouds

### Unigrams Word Cloud:
```{r, eval=FALSE}

par(mar=rep(0,4))
wordcloud(uber_vocab$term, uber_vocab$term_count, scale=c(4,.5),min.freq=3,max.words=100, random.order=FALSE,rot.per=.15,
          colors=brewer.pal(8, "Paired"))
wordcloud(lyft_vocab$term, lyft_vocab$term_count, scale=c(4,.5),min.freq=3,max.words=100, random.order=FALSE, rot.per=.15,
          colors=brewer.pal(8, "Paired"))


```


### Bigrams Word Cloud:
```{r, eval=FALSE}

par(mar=rep(0,4))
wordcloud(bi_uber_vocab$term, bi_uber_vocab$term_count, scale=c(4,.5),min.freq=5,max.words=500, random.order=FALSE, rot.per =.15,
          colors=brewer.pal(8, "Paired"))
wordcloud(bi_lyft_vocab$term, bi_lyft_vocab$term_count,scale=c(4,.5),min.freq=5,max.words=500,random.order=FALSE, rot.per=.15,
          colors=brewer.pal(8, "Paired"))

``` 


### Trigrams Word Cloud:
```{r, eval=FALSE}

par(mar=rep(0,4))
wordcloud(tri_uber_vocab$term, tri_uber_vocab$term_count,scale=c(4,.5),min.freq=5,max.words=500,colors=brewer.pal(8, "Paired"))
wordcloud(tri_lyft_vocab$term, tri_lyft_vocab$term_count,scale=c(4,.5),min.freq=5,max.words=500,colors=brewer.pal(8, "Paired"))

```


### TF-IDF Word Cloud for Unigram:
```{r, eval=FALSE}

par(mar=rep(0,4))
wordcloud(tfidf_freq_uber$words, tfidf_freq_uber$freq, scale=c(4,.5),min.freq=5,max.words=500, family="Garamond", colors=brewer.pal(8, "Paired"))
wordcloud(tfidf_freq_lyft$words, tfidf_freq_lyft$freq, scale=c(4,.5),min.freq=5,max.words=500,colors=brewer.pal(8, "Paired"))


```


# Sentiment Analysis - Using "Bing" Lexicon

### Uber
```{r, eval=FALSE}

tidy_uber <- tidy(uber_dtm)
bing_lex <- get_sentiments("bing")

uber_bing <- inner_join(tidy_uber, bing_lex, by = c("column" = "word"))

uber_bing$sentiment_n <- ifelse(uber_bing$sentiment=="negative", -1, 1)

uber_bing$sentiment_score <- uber_bing$value*uber_bing$sentiment_n

sentiment_summary_uber <- uber_bing %>%
  group_by(row) %>%
  summarize(review_sentiment = sum(sentiment_score)) %>%
  arrange(desc(review_sentiment))

#Looking at most frequent positive and negative words
uber_pos <- filter(uber_bing, sentiment=="positive") %>%
  group_by(column)%>%
  summarise(freq=sum(value))


uber_neg <- filter(uber_bing, sentiment=="negative") %>%
  group_by(column)%>%
  summarise(freq=sum(value))

hist_uber <- ggplot(data = sentiment_summary_uber) +
  aes(x = review_sentiment) +
  geom_histogram(bins = 30, fill = "#0c4c8a") +
  labs(title = "Uber Sentiment Analysis using BING Lexicon",
    x = "Sentiment Score",
    y = "Frequency") +
  theme_gray()

ggplotly(hist_uber)

```

### Lyft
```{r, eval=FALSE}

tidy_lyft <- tidy(lyft_dtm)
bing_lex <- get_sentiments("bing")

lyft_bing <- inner_join(tidy_lyft, bing_lex, by = c("column" = "word"))

lyft_bing$sentiment_n <- ifelse(lyft_bing$sentiment=="negative", -1, 1)

lyft_bing$sentiment_score <- lyft_bing$value*lyft_bing$sentiment_n

sentiment_summary_lyft <- lyft_bing %>%
  group_by(row) %>%
  summarize(review_sentiment = sum(sentiment_score)) %>%
  arrange(desc(review_sentiment))

#Looking at most frequent positive and negative words
lyft_pos <- filter(lyft_bing, sentiment=="positive") %>%
  group_by(column)%>%
  summarise(freq=sum(value))

lyft_neg <- filter(lyft_bing, sentiment=="negative") %>%
  group_by(column)%>%
  summarise(freq=sum(value))

hist_lyft <- ggplot(data = sentiment_summary_lyft) +
  aes(x = review_sentiment) +
  geom_histogram(bins = 30, fill = "#ef562d") +
  labs(title = "Lyft Sentiment Analysis using BING Lexicon",
    x = "Sentiment Score",
    y = "Frequency") +
  theme_gray()

ggplotly(hist_lyft)


```

# Emotion Analysis - Using "NRC" Lexicon

### Uber
```{r, eval=FALSE}

nrc_lex <- get_sentiments("nrc")

uber_nrc <- inner_join(tidy_uber, nrc_lex, by = c("column" = "word"))

#Removing positive and negative sentiments
uber_nrc_noposneg <- uber_nrc[!(uber_nrc$sentiment %in% c("positive","negative")),]

uber_score <- uber_nrc_noposneg %>%
  group_by(sentiment) %>%
  summarise(n=n())

#Radar Chart
chartJSRadar(uber_score)


#Bar Chart
uber_nrc_plot <- ggplot(data=uber_score,aes(x=sentiment,y=n))+geom_bar(aes(fill=sentiment),stat = "identity")+
  theme(legend.position="none")+
  xlab("Sentiments")+ylab("Scores")+ggtitle("Uber Emotion Analysis using NRC")

ggplotly(uber_nrc_plot)

```

### Lyft
```{r, eval=FALSE}

lyft_nrc <- inner_join(tidy_lyft, nrc_lex, by = c("column" = "word"))

#Removing positive and negative sentiments
lyft_nrc_noposneg <- lyft_nrc[!(lyft_nrc$sentiment %in% c("positive","negative")),]

lyft_score <- lyft_nrc_noposneg %>%
  group_by(sentiment) %>%
  summarise(n=n())

#Radar Chart
chartJSRadar(lyft_score)

#Bar Chart
lyft_nrc_plot <- ggplot(data=lyft_score,aes(x=sentiment,y=n))+geom_bar(aes(fill=sentiment),stat = "identity")+
  theme(legend.position="none")+
  xlab("Sentiments")+ylab("Scores")+ggtitle("Lyft Emotion Analysis using NRC")

ggplotly(lyft_nrc_plot)


```

# Comparison and Commonality Clouds
```{r, eval=FALSE}

#Extrating text and combining it together
uber_text = paste(uber$text, collapse=" ")
lyft_text = paste(lyft$text, collapse=" ")

#Put everything in a single vector
both_drivers <- c(uber_text, lyft_text)

#Making a corpus of a vector source 
both_corpus <- VCorpus(VectorSource(both_drivers))

#Cleaning corpus - pre_processing
clean_corpus2 <- function(corpus){
  cleaned_corpus2 <- tm_map(corpus, content_transformer(replace_abbreviation))
  cleaned_corpus2 <- tm_map(cleaned_corpus2, content_transformer(tolower))
  cleaned_corpus2 <- tm_map(cleaned_corpus2, removePunctuation)
  cleaned_corpus2 <- tm_map(cleaned_corpus2, removeNumbers)
  cleaned_corpus2 <- tm_map(cleaned_corpus2, removeWords, stopwords("english"))
  custom_stop_words <- c("lyft","uber", "Lyft","Uber")
  cleaned_corpus2 <- tm_map(cleaned_corpus2, removeWords, custom_stop_words)
  cleaned_corpus2 <- tm_map(cleaned_corpus2, stripWhitespace)
  return(cleaned_corpus2)
}

#Apply the clean_corpus function 
cleaned_both_corpus <- clean_corpus2(both_corpus)

#TDM
TDM_both <- TermDocumentMatrix(cleaned_both_corpus)
TDM_both_m <- as.matrix(TDM_both)

#Commonality Cloud
par(mar=rep(0,4))
commonality.cloud(TDM_both_m,colors=brewer.pal(8, "Dark2"),max.words = 500, random.order=FALSE)


#Comparison Cloud 
TDM_both <- TermDocumentMatrix(cleaned_both_corpus)
colnames(TDM_both) <- c("Uber","Lyft")
TDM_both_m <- as.matrix(TDM_both)
par(mar=rep(0,4))
comparison.cloud(TDM_both_m,colors=brewer.pal(8, "Dark2"),max.words = 500, random.order=FALSE)



```

